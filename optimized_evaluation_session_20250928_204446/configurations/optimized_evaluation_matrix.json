[
  {
    "combo_id": "G270_1_C5_c045afc7",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_high_capacity",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C10_1eb098b3",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_high_capacity",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C5_8d37cc11",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_fast",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C10_6b58128f",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_fast",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C5_791f5137",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_precision",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C10_e6325f52",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_precision",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C5_83b07fc4",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_high_capacity",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C10_4197c501",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_high_capacity",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C5_b0270e4d",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_precision",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C10_2f890961",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_precision",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C5_3ceff6bc",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_fast",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C10_f4f41145",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_fast",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C5_8fd68d36",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_fast",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C10_1c82059f",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_fast",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C5_3d0bb345",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_precision",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C10_de4c935c",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_precision",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C5_698a11a4",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_high_capacity",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C10_4c1e9d50",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_high_capacity",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C5_f60dfea0",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_high_capacity",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C10_276f4335",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_high_capacity",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C5_8cd651e6",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_baseline",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C10_252e94fc",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_baseline",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C5_66975eac",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_high_capacity",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C10_f6af82e7",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_high_capacity",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C5_c47fe076",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_triage_optimized",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C10_adf4300d",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_triage_optimized",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C5_82c3b554",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_precision",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C10_7f266177",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_precision",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C5_54daa704",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_high_capacity",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C10_5dc97882",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_high_capacity",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C5_b8b9e35c",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_conservative",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C10_1244063d",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_conservative",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C5_b34a5389",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_conservative",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C10_606601a6",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_conservative",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C5_15786e25",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_conservative",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C10_7082dba3",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_conservative",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C5_300aa1ea",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_precision",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C10_341b4b3c",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_precision",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C5_fac6a344",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_fast",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C10_063e1392",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_fast",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C5_6b449a21",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_triage_optimized",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C10_4b69fa3f",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_triage_optimized",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C5_07e2b3f7",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_baseline",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C10_34fe17fe",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_baseline",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C5_6e26c1ab",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_triage_optimized",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C10_bc05af11",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_triage_optimized",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C5_8a64a951",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_baseline",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C10_278e2699",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_baseline",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C5_58ee2cac",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_triage_optimized",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C10_2312f398",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_triage_optimized",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C5_fe799d11",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_baseline",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C10_4d3e8b41",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_baseline",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C5_9be15a85",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_triage_optimized",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C10_ed71067a",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_triage_optimized",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C5_f4483170",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_fast",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C10_54ce2ab5",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_fast",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C5_7707bd86",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_conservative",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C10_42e4ff8c",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_conservative",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C5_970e94b2",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_conservative",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C10_0cb135ad",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_conservative",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C5_c56f8f41",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_baseline",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C10_3ae5cc3e",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_baseline",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C5_0446ad67",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_conservative",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C10_454e7123",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_conservative",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C5_d033d640",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_precision",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C10_0dfbd2ee",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_precision",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C5_1df7f7f9",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_baseline",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C10_fdbe3311",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_baseline",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C5_704bb96e",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_triage_optimized",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C10_f9c8106c",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_triage_optimized",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C5_4d1bc6ec",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_fast",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C10_0ea4ed6c",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_fast",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C5_0a264b2a",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_high_capacity_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C10_60c72121",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_high_capacity_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C5_b9494d71",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_high_capacity_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C10_694449bd",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_high_capacity_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C5_9fd3f2cb",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_performance_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C10_aa8803e0",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_performance_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C5_3407b030",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_balanced_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C10_c682d0c2",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_balanced_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C5_5642698c",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_balanced_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C10_3c5321fa",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_balanced_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C5_5699ae4a",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_balanced_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C10_382281b0",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_balanced_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C5_5e4b7efd",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_performance_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C10_dd1ed125",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_performance_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C5_16795f9b",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_high_capacity_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C10_4fb9817d",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_high_capacity_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C5_6bd42ab4",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_ultra_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C10_364d1b63",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_ultra_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C5_e9a0ab44",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_high_capacity_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_214935/safe_config_safe_triage_Gemma-270M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C10_fc812da4",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_high_capacity_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_214935/safe_config_safe_triage_Gemma-270M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C5_52ee1c8a",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_ultra_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C10_7e4832fb",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_ultra_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C5_a4eb7ad3",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_ultra_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C10_2753556f",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_ultra_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C5_de9051fa",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_performance_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C10_95fcb392",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_performance_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C5_d001e2cd",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_high_capacity_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C10_692cecd1",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_high_capacity_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C5_80b316f5",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_performance_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C10_861985a7",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_performance_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C5_bbee4b37",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_balanced_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_100256/safe_config_safe_triage_SmolLM2-360M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C10_c76754ec",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_balanced_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_100256/safe_config_safe_triage_SmolLM2-360M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C5_4d6eb2c0",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_balanced_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C10_8eee4699",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_balanced_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C5_3a482fea",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_balanced_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C10_c1b77866",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_balanced_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C5_b7855904",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_ultra_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C10_e85cf6f4",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_ultra_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C5_78bff135",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_high_capacity_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C10_9ef511e9",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_high_capacity_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C5_b38f0eb5",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_ultra_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_093118/safe_config_safe_triage_SmolLM2-360M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C10_2e1ec9ba",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_ultra_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_093118/safe_config_safe_triage_SmolLM2-360M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C5_c58502d9",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_ultra_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_1_C10_29a7fa7b",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_ultra_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C5_b0fe2dcc",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_performance_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_1_C10_1c2ed696",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_performance_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C5_9d57b070",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_performance_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_1_C10_7f452be7",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.595,
      "pass_at_10": 0.77,
      "avg_retrieval_time": 0.0321,
      "peak_memory_mb": 1.8,
      "config_id": "pass5_1_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_performance_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_3d652561",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_high_capacity",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_90c8b73b",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_high_capacity",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_a61053c2",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_fast",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_d4e45f98",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_fast",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_7414c8d2",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_precision",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_30c66c55",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_precision",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_7c6fb700",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_high_capacity",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_86159cdf",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_high_capacity",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_c22bf365",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_precision",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_e735134e",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_precision",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_348b0ee2",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_fast",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_ed106ac7",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_fast",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_096086ca",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_fast",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_7b15e8a3",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_fast",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_b644e001",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_precision",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_92b9084d",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_precision",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_29a4825f",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_high_capacity",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_61e996e8",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_high_capacity",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_d88bc25a",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_high_capacity",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_d7bba9e1",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_high_capacity",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_4ecefb02",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_baseline",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_9e43e769",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_baseline",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_6d4d351b",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_high_capacity",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_97eebb22",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_high_capacity",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_49767484",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_triage_optimized",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_482f792a",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_triage_optimized",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_76569bf1",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_precision",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_eeb51519",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_precision",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_f99de8a0",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_high_capacity",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_43fc8df7",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_high_capacity",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_49f75b04",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_conservative",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_8a10611f",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_conservative",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_a67b5a33",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_conservative",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_4c5beec7",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_conservative",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_b55982fc",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_conservative",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_efc76714",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_conservative",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_1ef5a681",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_precision",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_de368e12",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_precision",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_585d8cde",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_fast",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_f741aad8",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_fast",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_f5ea8e93",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_triage_optimized",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_f2e818f3",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_triage_optimized",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_7147d239",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_baseline",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_d8932b6c",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_baseline",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_6acbec55",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_triage_optimized",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_b4ddf0ea",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_triage_optimized",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_8858c9e5",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_baseline",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_f7cfc0d4",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_baseline",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_60b1dd38",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_triage_optimized",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_295b33bf",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_triage_optimized",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_5cc3f14d",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_baseline",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_ee829ebc",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_baseline",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_8a5d9d0b",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_triage_optimized",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_e29a6873",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_triage_optimized",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_ae684a23",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_fast",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_a48495d5",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_fast",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_94aa6af4",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_conservative",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_33518d7d",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_conservative",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_a96511c2",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_conservative",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_b59aa4f4",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_conservative",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_18edc0c2",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_baseline",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_b6f0c872",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_baseline",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_8f527d2e",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_conservative",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_bca3874a",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_conservative",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_921571f0",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_precision",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_13fee9ea",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_precision",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_3f7e807d",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_baseline",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_15ba05e5",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_baseline",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_869b86ff",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_triage_optimized",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_25be7525",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_triage_optimized",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_415ea637",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_fast",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_477ccaa0",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_fast",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_e1dc1f32",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_high_capacity_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_f9f18031",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_high_capacity_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_b57b0f19",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_high_capacity_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_ff335f91",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_high_capacity_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_a2cc296f",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_performance_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_6c061fb1",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_performance_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_a1a7e4a9",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_balanced_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_ed385564",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_balanced_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_5a0725bf",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_balanced_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_53cd6239",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_balanced_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_e8aaf17e",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_balanced_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_6ff876b9",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_balanced_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_8b9e5ce3",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_performance_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_8b4c54f6",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_performance_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_0d5e4003",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_high_capacity_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_b098d94d",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_high_capacity_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_7dbb6139",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_ultra_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_6301cd18",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_ultra_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_84bdf428",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_high_capacity_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_214935/safe_config_safe_triage_Gemma-270M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_23651e18",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_high_capacity_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_214935/safe_config_safe_triage_Gemma-270M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_dcf07f5f",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_ultra_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_1aba8557",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_ultra_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_cf497dbd",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_ultra_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_8382d0a4",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_ultra_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_a4fa1102",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_performance_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_cf9c8f8e",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_performance_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_64aa608e",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_high_capacity_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_1f4fe9b6",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_high_capacity_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_219b3bf2",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_performance_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_f5a18438",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_performance_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_ddd797ed",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_balanced_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_100256/safe_config_safe_triage_SmolLM2-360M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_65b89ea8",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_balanced_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_100256/safe_config_safe_triage_SmolLM2-360M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_f6e0065f",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_balanced_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_9495c680",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_balanced_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_112fedce",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_balanced_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_ff0ad1e7",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_balanced_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_30b17023",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_ultra_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_da7c3614",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_ultra_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_426bdd30",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_high_capacity_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_eb5d4582",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_high_capacity_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_0de669b5",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_ultra_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_093118/safe_config_safe_triage_SmolLM2-360M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_9d5b7344",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_ultra_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_093118/safe_config_safe_triage_SmolLM2-360M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_62e0f326",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_ultra_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_261e3b1b",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_ultra_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_fe13c307",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_performance_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_3b1e9e84",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_performance_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_958438fe",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_performance_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_04eae486",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.59,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 30.3,
      "config_id": "pass5_2_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_performance_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_bb1ae7d7",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_high_capacity",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_1d2957f4",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_high_capacity",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_49034371",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_fast",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_edb01928",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_fast",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_9d3bd25e",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_precision",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_7e2b0580",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_precision",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_fb50f2fc",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_high_capacity",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_6dd0578e",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_high_capacity",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_25a052c8",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_precision",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_c6e49e97",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_precision",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_857d1487",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_fast",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_7c96763b",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_fast",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_ef6a0b02",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_fast",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_298f38dd",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_fast",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_cd076518",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_precision",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_650c6dba",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_precision",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_f914f343",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_high_capacity",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_d336fa59",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_high_capacity",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_8c78caaf",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_high_capacity",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_a0dc3014",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_high_capacity",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_e2729957",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_baseline",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_c5e8a82d",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_baseline",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_f714dc5c",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_high_capacity",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_88f1c3f8",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_high_capacity",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_e686291e",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_triage_optimized",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_163febcf",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_triage_optimized",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_2d51ae5f",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_precision",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_fdbfa205",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_precision",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_e0035214",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_high_capacity",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_367063f9",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_high_capacity",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_00221a00",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_conservative",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_4ac26215",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_conservative",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_9a0b3f5f",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_conservative",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_734c1e13",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_conservative",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_aaa4a81b",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_conservative",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_eade1832",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_conservative",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_53ab3d81",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_precision",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_5197e886",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_precision",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_d00929ef",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_fast",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_fe018ee6",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_fast",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_d524639a",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_triage_optimized",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_e817a565",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_triage_optimized",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_1686f6c2",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_baseline",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_4646ce47",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_baseline",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_2ec2e1a0",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_triage_optimized",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_599576c6",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_triage_optimized",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_824b0d8e",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_baseline",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_ea061cd6",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_baseline",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_1055d21f",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_triage_optimized",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_07946014",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_triage_optimized",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_5eb15812",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_baseline",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_50c43ee4",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_baseline",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_f09ea3c1",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_triage_optimized",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_64083934",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_triage_optimized",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_f3be6df2",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_fast",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_3b203a29",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_fast",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_85691a75",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_conservative",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_285af201",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_conservative",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_7d1a8cdb",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_conservative",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_5bd2802f",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_conservative",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_c080d922",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_baseline",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_72b39cb6",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_baseline",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_75474523",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_conservative",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_e21d9137",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_conservative",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_af4e4f52",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_precision",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_453ce4af",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_precision",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_78c920f1",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_baseline",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_e848bd26",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_baseline",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_a8b3ff73",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_triage_optimized",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_1cf27a3b",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_triage_optimized",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_df44a705",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_fast",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_d1f86fde",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_fast",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_8334f438",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_high_capacity_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_b0c3ea1a",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_high_capacity_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_00a094e2",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_high_capacity_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_1bcb45da",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_high_capacity_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_a89066fc",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_performance_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_7be56477",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_performance_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_dc9b0a97",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_balanced_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_5200ed30",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_balanced_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_ee60ff03",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_balanced_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_1def57ad",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_balanced_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_2c9a687c",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_balanced_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_4c0cb848",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_balanced_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_f443b5e1",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_performance_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_58a00c72",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_performance_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_bb53e171",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_high_capacity_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_674b9e32",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_high_capacity_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_38c28c64",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_ultra_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_128cfb1a",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_ultra_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_4571f84a",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_high_capacity_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_214935/safe_config_safe_triage_Gemma-270M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_c487acac",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_high_capacity_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_214935/safe_config_safe_triage_Gemma-270M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_e6b58405",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_ultra_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_edde0705",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_ultra_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_6650cf62",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_ultra_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_d3b77aa8",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_ultra_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_4ee28a90",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_performance_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_ae2be7e1",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_performance_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_0335d759",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_high_capacity_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_74b858dd",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_high_capacity_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_9230642e",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_performance_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_dbc195c2",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_performance_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_e4827ffe",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_balanced_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_100256/safe_config_safe_triage_SmolLM2-360M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_51b2af07",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_balanced_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_100256/safe_config_safe_triage_SmolLM2-360M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_b9c478f5",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_balanced_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_eb72d965",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_balanced_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_69179247",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_balanced_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_94a82773",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_balanced_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_855a4451",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_ultra_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_93cd0be5",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_ultra_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_3b259a66",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_high_capacity_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_2a1f8247",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_high_capacity_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_ff2f5246",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_ultra_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_093118/safe_config_safe_triage_SmolLM2-360M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_d3f4c854",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_ultra_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_093118/safe_config_safe_triage_SmolLM2-360M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_72c42a69",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_ultra_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_2fd4b727",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_ultra_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_056f4f4d",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_performance_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_c8685c0a",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_performance_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_5556eecf",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_performance_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_b750a57f",
    "rag_config": {
      "chunking_method": "contextual_sentence_c1024_o2_tinfoil",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.525,
      "pass_at_10": 0.635,
      "avg_retrieval_time": 0.461,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_3_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_performance_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C5_1e68e5af",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_high_capacity",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C10_5cfda310",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_high_capacity",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C5_76d488d1",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_fast",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C10_e3005da4",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_fast",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C5_018e2d75",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_precision",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C10_abc2be7d",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_precision",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C5_570ad794",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_high_capacity",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C10_2de1ed82",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_high_capacity",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C5_e500d5e7",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_precision",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C10_638ca271",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_precision",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C5_b2219496",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_fast",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C10_083f0a11",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_fast",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C5_cb538c41",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_fast",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C10_66a7786e",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_fast",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C5_53dcfb34",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_precision",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C10_d387c5d5",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_precision",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C5_8e91aa2a",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_high_capacity",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C10_0b48ef91",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_high_capacity",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C5_0020bb9b",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_high_capacity",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C10_50899b94",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_high_capacity",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C5_b159b9fd",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_baseline",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C10_0ace2cbd",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_baseline",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C5_745f5daf",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_high_capacity",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C10_f314788e",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_high_capacity",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C5_07a89a45",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_triage_optimized",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C10_75fa20cc",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_triage_optimized",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C5_3b06f83c",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_precision",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C10_7202bcea",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_precision",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C5_034047a2",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_high_capacity",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C10_cd3e40d3",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_high_capacity",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C5_c7d4b881",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_conservative",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C10_a5b22f4b",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_conservative",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C5_4b9a09da",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_conservative",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C10_cffbbd0a",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_conservative",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C5_64368058",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_conservative",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C10_c632bef6",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_conservative",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C5_330c4cb8",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_precision",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C10_0962b5db",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_precision",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C5_2ee616a2",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_fast",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C10_feb2dafe",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_fast",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C5_d79a7af9",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_triage_optimized",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C10_b7dbe04c",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_triage_optimized",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C5_c5eaa28e",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_baseline",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C10_540a640c",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_baseline",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C5_de77067f",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_triage_optimized",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C10_0a1c2020",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_triage_optimized",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C5_4338168c",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_baseline",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C10_6d822fd2",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_baseline",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C5_fd5b96f6",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_triage_optimized",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C10_d4ff233d",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_triage_optimized",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C5_29d6e953",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_baseline",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C10_67a8dcf9",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_baseline",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C5_59ae1d44",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_triage_optimized",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C10_70a40587",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_triage_optimized",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C5_41fa3266",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_fast",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C10_ed9a2527",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_fast",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C5_1206a8ac",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_conservative",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C10_8a021fa6",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_conservative",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C5_335141dc",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_conservative",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C10_4aa23627",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_conservative",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C5_e7faf66e",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_baseline",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C10_09603296",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_baseline",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C5_0c19d23d",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_conservative",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C10_169d65a2",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_conservative",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C5_1d96da98",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_precision",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C10_e2e25ec9",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_precision",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C5_2314b012",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_baseline",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C10_c52ba795",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_baseline",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C5_30c03a46",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_triage_optimized",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C10_298e6b9c",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_triage_optimized",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C5_7e01537c",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_fast",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C10_63f1cf40",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_fast",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C5_29b11cce",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_high_capacity_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C10_da3d6ecc",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_high_capacity_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C5_894db36c",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_high_capacity_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C10_2a2a89c3",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_high_capacity_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C5_93259baa",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_performance_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C10_1dec8608",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_performance_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C5_e0c5d4bb",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_balanced_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C10_622af45b",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_balanced_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C5_4d2e5083",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_balanced_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C10_251d3329",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_balanced_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C5_ee0b5348",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_balanced_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C10_960a8c5e",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_balanced_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C5_ccfc7477",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_performance_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C10_e2d9e0d9",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_performance_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C5_c5a7a17a",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_high_capacity_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C10_7373782e",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_high_capacity_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C5_d6c17e4a",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_ultra_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C10_8e5feda8",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_ultra_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C5_7b36a7e2",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_high_capacity_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_214935/safe_config_safe_triage_Gemma-270M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C10_bc00a9b0",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_high_capacity_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_214935/safe_config_safe_triage_Gemma-270M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C5_8edf21e4",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_ultra_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C10_d1aaa8f1",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_ultra_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C5_1e076a09",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_ultra_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C10_e74818a5",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_ultra_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C5_9da0e85c",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_performance_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C10_63a6e6a5",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_performance_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C5_68822800",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_high_capacity_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C10_47ad075a",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_high_capacity_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C5_d1f527de",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_performance_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C10_2d3e6dc4",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_performance_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C5_946067f6",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_balanced_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_100256/safe_config_safe_triage_SmolLM2-360M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C10_2dfa33e7",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_balanced_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_100256/safe_config_safe_triage_SmolLM2-360M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C5_b50058e4",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_balanced_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C10_93c75bf0",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_balanced_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C5_3b1cbe85",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_balanced_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C10_8e1f8d80",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_balanced_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C5_0a355d52",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_ultra_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C10_4b403927",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_ultra_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C5_3ff34809",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_high_capacity_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C10_231e8e5b",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_high_capacity_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C5_7def4311",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_ultra_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_093118/safe_config_safe_triage_SmolLM2-360M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C10_c3e4b430",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_ultra_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_093118/safe_config_safe_triage_SmolLM2-360M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C5_a0c3b775",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_ultra_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_4_C10_eff0ffd3",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_ultra_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C5_54567501",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_performance_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_4_C10_f4228ec0",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_performance_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C5_101e0a9e",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 5,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_performance_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_4_C10_b460f6e7",
    "rag_config": {
      "chunking_method": "contextual_fixed_c512_o100",
      "retrieval_type": "contextual_rag",
      "bias_config": "diverse",
      "chunk_limit": 10,
      "pass_at_5": 0.52,
      "pass_at_10": 0.65,
      "avg_retrieval_time": 1.059,
      "peak_memory_mb": 0.0,
      "config_id": "pass5_4_contextu_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_performance_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_c3ea4298",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_high_capacity",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_13163b99",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_high_capacity",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_0dee7e2f",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_fast",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_5edd0687",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_fast",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_0e1f3772",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_precision",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_98d48839",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_precision",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_e36ab60e",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_high_capacity",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_70b58e0b",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_high_capacity",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_f3d88941",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_precision",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_d0ef4e28",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_precision",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_afb7380e",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_fast",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_a17e4bc1",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_fast",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_f13d9078",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_fast",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_bc0ad96d",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_fast",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_6da5564b",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_precision",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_c9042101",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_precision",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_588ba8e6",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_high_capacity",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_5823f76d",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_high_capacity",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_1e07e1dd",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_high_capacity",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_de5e4957",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_high_capacity",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_8b230259",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_baseline",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_94a91585",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_baseline",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_722fcff9",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_high_capacity",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_28e15b3f",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_high_capacity",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_88c1e208",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_triage_optimized",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_fe6cc993",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_triage_optimized",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_20248ed7",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_precision",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_dfaf5fb1",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_precision",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_ea4557ae",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_high_capacity",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_80eefe65",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_high_capacity",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_9c9a58a8",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_conservative",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_91ecf319",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_conservative",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_874c40e4",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_conservative",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_06ce5a8f",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_conservative",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_d2b5b1db",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_conservative",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_adc9a89f",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_conservative",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_10ca6393",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_precision",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_9f0e417b",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_precision",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_04e31d2d",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_fast",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_3b481f4d",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_fast",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_c0da25af",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_triage_optimized",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_7c02ea50",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_triage_optimized",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_150d0cdc",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_baseline",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_13237295",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_baseline",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_cecb7522",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_triage_optimized",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_10aaf31e",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_triage_optimized",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_2027ece4",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_baseline",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_e3048b17",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_baseline",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_6aee2f23",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_triage_optimized",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_dc694bb5",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_triage_optimized",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_9023248b",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_baseline",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_a3af2f18",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_baseline",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_8016a5f6",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_triage_optimized",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_9d74e34e",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_triage_optimized",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_2b77463e",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_fast",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_70924f17",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_fast",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_99fac91d",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_conservative",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_856f0eba",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_conservative",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_2cf1786e",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_conservative",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_faadccac",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_conservative",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_65d4c256",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_baseline",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_04a27426",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_baseline",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_46056636",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_conservative",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_7e7611a5",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_conservative",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_11778e00",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_precision",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_4b754030",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_precision",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_f1cc2632",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_baseline",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_b5581551",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_baseline",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_6a21fcfa",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_triage_optimized",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_500c5b3d",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_triage_optimized",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_d37b767a",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_fast",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_3d3b89ea",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_fast",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_2554a798",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_high_capacity_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_2667673e",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_high_capacity_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_f8753c33",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_high_capacity_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_85452d97",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_high_capacity_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_df518366",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_performance_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_31a66b0c",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_performance_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_0f52b5b1",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_balanced_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_39280fd9",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_balanced_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_a7f4bc0d",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_balanced_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_fe35fcc7",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_balanced_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_8794bf77",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_balanced_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_9bc1ddd9",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_balanced_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_a1065d72",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_performance_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_661b662d",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_performance_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_d4580473",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_high_capacity_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_e4d3e51c",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_high_capacity_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_f7edb76b",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_ultra_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_30ed3fee",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_ultra_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_c1e6b127",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_high_capacity_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_214935/safe_config_safe_triage_Gemma-270M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_83bfcb2b",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_high_capacity_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_214935/safe_config_safe_triage_Gemma-270M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_ec98b4e6",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_ultra_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_929226cf",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_ultra_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_fe6e87c7",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_ultra_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_bb5cca5e",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_ultra_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_57467953",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_performance_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_eb6a07ad",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_performance_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_3e7c9b3b",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_high_capacity_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_ba5c08ac",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_high_capacity_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_8615a97e",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_performance_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_948208a7",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_performance_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_721d6535",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_balanced_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_100256/safe_config_safe_triage_SmolLM2-360M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_b3e49d90",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_balanced_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_100256/safe_config_safe_triage_SmolLM2-360M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_663da35e",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_balanced_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_31c589e8",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_balanced_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_d4a5684e",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_balanced_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_4e5485d2",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_balanced_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_3c14c9ee",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_ultra_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_6eb97ffa",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_ultra_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_59e42454",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_high_capacity_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_b8b6068b",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_high_capacity_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_9fd229f9",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_ultra_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_093118/safe_config_safe_triage_SmolLM2-360M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_b27cff1d",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_ultra_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_093118/safe_config_safe_triage_SmolLM2-360M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C5_ee5ed909",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_ultra_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_2_C10_f03f3cf1",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_ultra_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C5_ab4bb71d",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_performance_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_2_C10_29bcd383",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_performance_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C5_8389a83a",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_performance_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_2_C10_26381238",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "contextual_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.39,
      "pass_at_10": 0.75,
      "avg_retrieval_time": 0.0352,
      "peak_memory_mb": 0.0,
      "config_id": "pass10_2_structur_contex"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_performance_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_3e6d3c40",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_high_capacity",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_a6932e97",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_high_capacity",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_85a8d764",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_fast",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_c39b209a",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_fast",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_7b5b0a60",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_precision",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_651985d9",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_precision",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_b253acc3",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_high_capacity",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_24f60e49",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_high_capacity",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_ec59b18a",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_precision",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_1c1c57ba",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_precision",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_0592568f",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_fast",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_f307e5f4",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_fast",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_609a5b66",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_fast",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_650450de",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_fast",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_96f8efe4",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_precision",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_b672987a",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_precision",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_c224e257",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_high_capacity",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_2e318dd1",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_high_capacity",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_43532506",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_high_capacity",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_6cc6e693",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_high_capacity",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_35ce96b4",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_baseline",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_92ffc11f",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_baseline",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_231adef9",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_high_capacity",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_dd021ab3",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_high_capacity",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_499b3318",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_triage_optimized",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_e9dfb48e",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_triage_optimized",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_cc1dc69b",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_precision",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_81488643",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_precision",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_d0d385e0",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_high_capacity",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_10b23789",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_high_capacity",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_high_capacity",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_8bit_medical_high_capacity.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 32,
          "dropout": 0.05,
          "scale": 30.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_cf5510ac",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_conservative",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_8d7c83e5",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_conservative",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_03b614eb",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_conservative",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_da13a3f2",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_conservative",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_da0bb590",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_conservative",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_492120fb",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_conservative",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_e68359b4",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_precision",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_1f0d8309",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_precision",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_0c777a23",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_fast",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_feb4ea32",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_fast",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_23489a65",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_triage_optimized",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_6cd95a83",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_triage_optimized",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_6b8b690e",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_baseline",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_199ff338",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_baseline",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_6dc237d7",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_triage_optimized",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_122f3ba7",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_triage_optimized",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-360M_8bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_5170fc02",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_baseline",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_e041bcf4",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_baseline",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_8bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_8bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_1120cf31",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_triage_optimized",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_2154e16e",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_triage_optimized",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_0496735d",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_baseline",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_b9cf9148",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_baseline",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_d047fb24",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_triage_optimized",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_acaef89b",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_triage_optimized",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_137d4bd2",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_fast",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_0fdd46e2",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_fast",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_9656e27f",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_conservative",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_c2f49840",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_conservative",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_3490b774",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_conservative",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_fa5e9a91",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_conservative",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_c4ace072",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_baseline",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_9aa76b66",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_baseline",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_SmolLM2-135M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_6bcc7603",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_conservative",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_6e1ec53a",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_conservative",
      "model_name": "SmolLM2-360M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-360M_4bit_medical_conservative",
        "batch_size": 2,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-360M_4bit_medical_conservative.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.1,
          "scale": 15.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_43459a51",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_precision",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_639471e2",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_precision",
      "model_name": "SmolLM2-135M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_SmolLM2-135M_8bit_medical_precision",
        "batch_size": 4,
        "config": "triage_experiment_results_20250926_081126/config_triage_SmolLM2-135M_8bit_medical_precision.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.05,
          "scale": 25.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_0cbaf8bf",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_baseline",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_a71fb0d4",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_medical_baseline",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_medical_baseline",
        "batch_size": 4,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_medical_baseline.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 3e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_cd4320ea",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_triage_optimized",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_16a8b4bd",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_4bit_triage_optimized",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_4bit_triage_optimized",
        "batch_size": 6,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_4bit_triage_optimized.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 800,
        "learning_rate": 4e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.02,
          "scale": 22.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_4113a1ef",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_fast",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_a3ee8d9b",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/triage_adapters/adapter_triage_Gemma-270M_8bit_medical_fast",
      "model_name": "Gemma-270M",
      "adapter_type": "standard",
      "base_config": {
        "adapter_path": "triage_adapters/adapter_triage_Gemma-270M_8bit_medical_fast",
        "batch_size": 8,
        "config": "triage_experiment_results_20250925_231645/config_triage_Gemma-270M_8bit_medical_fast.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 600,
        "learning_rate": 5e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.0,
          "scale": 20.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_1e29bba0",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_high_capacity_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_82ffef46",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_high_capacity_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_b9dd4633",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_high_capacity_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_1259bd3a",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_high_capacity_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_cfb74c81",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_performance_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_0097201f",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_performance_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_e3bcc755",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_balanced_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_2aea464c",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_balanced_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_468357f7",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_balanced_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_3d80a83d",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_balanced_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_2935cf1d",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_balanced_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_410f7788",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_balanced_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_140f7b6b",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_performance_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_5d3b6432",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_performance_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_34b5d627",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_high_capacity_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_5aaa56d9",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_high_capacity_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_18655d2c",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_ultra_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_162cbb9d",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_ultra_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_080043a7",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_high_capacity_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_214935/safe_config_safe_triage_Gemma-270M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_c820342c",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_high_capacity_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_214935/safe_config_safe_triage_Gemma-270M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_71b1dd78",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_ultra_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_23630e74",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_ultra_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_162630/safe_config_safe_triage_Gemma-270M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_15de0667",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_ultra_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_ba825383",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_ultra_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_87ba7d9e",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_performance_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_5112be02",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_performance_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_fb6442c6",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_high_capacity_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_64f69dd4",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_high_capacity_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_e9540253",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_performance_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_89078328",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_performance_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_4bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_4bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_8446f0a2",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_balanced_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_100256/safe_config_safe_triage_SmolLM2-360M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_d7889032",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_balanced_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_100256/safe_config_safe_triage_SmolLM2-360M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_2725ecd9",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_balanced_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_e990dc63",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_balanced_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_a7abbe0b",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_balanced_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_628fd575",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_balanced_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_balanced_safe",
        "batch_size": 4,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_balanced_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1200,
        "learning_rate": 1e-05,
        "lora_parameters": {
          "rank": 8,
          "dropout": 0.1,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.96,
          "safety_priority": "high",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_3996a8a5",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_ultra_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_bb64d922",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_ultra_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_4bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_4bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_4bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_b68cc745",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_high_capacity_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_8ee2e226",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_high_capacity_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_high_capacity_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_8bit_high_capacity_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1400,
        "learning_rate": 8e-06,
        "lora_parameters": {
          "rank": 16,
          "dropout": 0.12,
          "scale": 8.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_5d48562d",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_ultra_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_093118/safe_config_safe_triage_SmolLM2-360M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_a2987c51",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_ultra_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_093118/safe_config_safe_triage_SmolLM2-360M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C5_4ef391ba",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_ultra_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "G270_3_C10_467e6f93",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_ultra_safe",
      "model_name": "Gemma-270M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_Gemma-270M_8bit_ultra_safe",
        "batch_size": 2,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_Gemma-270M_8bit_ultra_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1500,
        "learning_rate": 5e-06,
        "lora_parameters": {
          "rank": 4,
          "dropout": 0.15,
          "scale": 2.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/gemma-270m-mlx_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.98,
          "safety_priority": "maximum",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C5_668c7414",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_performance_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S360_3_C10_58455c2c",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_performance_safe",
      "model_name": "SmolLM2-360M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-360M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-360M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-360M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C5_f6b364b0",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 5,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_performance_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  },
  {
    "combo_id": "S135_3_C10_248475da",
    "rag_config": {
      "chunking_method": "structured_agent_tinfoil_medical",
      "retrieval_type": "pure_rag",
      "bias_config": "healthify_focused",
      "chunk_limit": 10,
      "pass_at_5": 0.385,
      "pass_at_10": 0.73,
      "avg_retrieval_time": 0.0017,
      "peak_memory_mb": 142.2,
      "config_id": "pass10_3_structur_pure_r"
    },
    "adapter_config": {
      "adapter_path": "/Users/choemanseung/789/hft/safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_performance_safe",
      "model_name": "SmolLM2-135M",
      "adapter_type": "safety",
      "base_config": {
        "adapter_path": "safety_triage_adapters/adapter_safe_triage_SmolLM2-135M_8bit_performance_safe",
        "batch_size": 6,
        "config": "safety_triage_results_20250926_114751/safe_config_safe_triage_SmolLM2-135M_8bit_performance_safe.json",
        "data": "./Final_dataset/final_triage_dialogues_mlx",
        "fine_tune_type": "lora",
        "grad_checkpoint": false,
        "iters": 1000,
        "learning_rate": 2e-05,
        "lora_parameters": {
          "rank": 12,
          "dropout": 0.08,
          "scale": 12.0
        },
        "lr_schedule": null,
        "mask_prompt": true,
        "max_seq_length": 2048,
        "model": "/Users/choemanseung/789/hft/mlx_models/SmolLM2-135M-Instruct-MLX_8bit",
        "num_layers": 16,
        "optimizer": "adamw",
        "optimizer_config": {
          "adamw": {
            "weight_decay": 0.01
          }
        },
        "project_name": null,
        "report_to": null,
        "resume_adapter_file": null,
        "safety_config": {
          "class_weights": {
            "ED": 9.324834749763928,
            "GP": 0.8754432624113475,
            "HOME": 5.5790960451977405
          },
          "cost_matrix": {
            "ED_GP": 100.0,
            "ED_HOME": 100.0,
            "GP_HOME": 10.0,
            "GP_ED": 2.0,
            "HOME_ED": 2.0,
            "HOME_GP": 3.0,
            "ED_ED": 1.0,
            "GP_GP": 1.0,
            "HOME_HOME": 1.0
          },
          "target_ed_recall": 0.95,
          "safety_priority": "moderate",
          "mc_dropout_samples": 100,
          "uncertainty_threshold": 0.3
        },
        "save_every": 100,
        "seed": 42,
        "steps_per_eval": 200,
        "steps_per_report": 50,
        "test": true,
        "test_batches": -1,
        "train": true,
        "val_batches": -1,
        "wandb": null,
        "warmup_ratio": 0.1
      }
    }
  }
]