# F. Model Fine-Tuning

The fine-tuning process began with the selection of three base models suited for resource-constrained environments: SmolLM2-135M, SmolLM2-360M, and Gemma-270M. These models were sourced from the Hugging Face Hub and prepared for efficient training on Apple Silicon hardware. The first step was to convert the pre-trained models into the MLX format using utilities from the mlx-lm library, a conversion necessary to take advantage of Metal Performance Shaders and the unified memory architecture of Apple Silicon. This enabled hardware-accelerated training and inference while remaining within resource limits. Following conversion, each model was quantized to both 4-bit and 8-bit precision, resulting in six distinct model variants that served as the basis for evaluating trade-offs between model size, computational efficiency, and task performance.

Fine-tuning was conducted using Quantized Low-Rank Adaptation (QLoRA), a parameter-efficient method that freezes the weights of the quantized base model and inserts lightweight, trainable low-rank matrices (adapters) into its layers. This drastically reduced the number of trainable parameters to between 0.1% and 0.4% of the base model size, making fine-tuning feasible on consumer-grade hardware without undermining the representational capacity of the base models. The experimental setup evaluated all six quantized model variants across ten distinct LoRA configurations: six standard configurations optimized for general medical triage performance, and four safety-enhanced configurations designed to enforce strict clinical safety constraints. This resulted in a total of sixty fine-tuning experiments.

The six standard LoRA configurations explored a range of adaptation strategies. The learning rate varied from 1×10⁻⁵ to 5×10⁻⁵, LoRA rank from 4 to 32, and LoRA alpha (scaling factor) from 15.0 to 30.0. Batch sizes ranged from 2 to 8, and dropout rates from 0.0 to 0.1, with training iterations between 600 and 1200. These configurations included a baseline approach (rank 8, learning rate 3×10⁻⁵), a high-precision variant (rank 16, learning rate 1×10⁻⁵), a fast-training option (rank 8, batch size 8, learning rate 5×10⁻⁵), a conservative low-rank setup (rank 4, learning rate 1×10⁻⁵), a high-capacity variant (rank 32, learning rate 2×10⁻⁵), and a triage-optimized configuration (rank 12, learning rate 4×10⁻⁵). All standard configurations employed the AdamW optimizer with a weight decay of 0.01 and a warmup ratio of 0.1 to ensure stable convergence.

The four safety-enhanced configurations implemented cost-sensitive learning with explicit penalties for dangerous misclassifications. A cost matrix was defined such that missing an emergency department (ED) case and triaging it to general practitioner (GP) or home care incurred a 100× penalty relative to correct classifications, while over-triaging to ED carried only a 2× penalty, reflecting the clinical principle that false positives are preferable to false negatives in emergency contexts. Class weights were set to ED=5.0×, GP=2.0×, and HOME=1.0×, prioritizing model sensitivity to critical cases. These configurations used more conservative learning rates (5×10⁻⁶ to 2×10⁻⁵), higher dropout rates (0.08 to 0.15) for Monte Carlo sampling, and lower LoRA alpha values (2.0 to 12.0) to prevent catastrophic forgetting. Training iterations ranged from 1000 to 1500, and each configuration specified a target ED recall threshold between 95% and 98% to enforce safety compliance.

Training was managed to operate within memory constraints while ensuring stable convergence. Dynamic batch sizing, ranging from 2 to 8 samples depending on model size and quantization level, was employed to balance throughput and memory usage. Comprehensive monitoring was integrated throughout the process: training, validation, and test loss were tracked in real time, with evaluations performed every 200 iterations and progress reported every 50 iterations. All experiments used a fixed random seed (42) for reproducibility, a maximum sequence length of 2048 tokens, and masked prompt training to compute loss only on model completions rather than input prompts.

Evaluation of the standard configurations focused on perplexity-based metrics and test accuracy. Final train loss, validation loss, test loss, and test accuracy were recorded for each experiment, alongside training time and memory utilization. The safety-enhanced configurations, in contrast, employed a comprehensive suite of safety-critical metrics. Per-class precision, recall, F1-score, and F2-score (a recall-weighted variant with β=2) were computed for ED, GP, and HOME triage decisions. F2-score was prioritized due to its emphasis on recall, which is critical for avoiding false negatives in emergency cases. Three hard safety constraints were enforced: ED recall ≥95%, ED F2-score ≥90%, and ED false negative rate ≤5%. Models failing any of these constraints were flagged as unsafe for deployment.

To quantify model uncertainty and identify high-risk predictions, Monte Carlo Dropout inference was performed during evaluation. Each test case was processed 50 to 100 times with dropout enabled, generating a distribution of predictions from which the majority prediction, confidence (frequency of the most common prediction), and uncertainty (entropy of the prediction distribution) were derived. Cases with uncertainty exceeding a threshold of 0.3 were flagged for manual review. This uncertainty quantification provided an additional layer of safety by identifying ambiguous cases where the model lacked confidence in its triage decision.

The final stage of the process evaluated the impact of 4-bit versus 8-bit quantization on both training efficiency and final model performance. The 4-bit quantized models achieved an approximate 50% reduction in memory footprint compared to their 8-bit counterparts (e.g., SmolLM2-360M reduced from ~720 MB to ~360 MB), enabling larger batch sizes and faster training iterations on the same hardware. Accuracy metrics on the validation and test sets indicated that performance degradation due to 4-bit quantization was minimal, typically less than 2% relative to the 8-bit models across all configurations. These findings validated the use of aggressive quantization, demonstrating that it provides substantial computational savings while maintaining accuracy levels suitable for safe deployment under resource-constrained conditions. The combination of QLoRA's parameter efficiency (reducing trainable parameters to <0.5% of the base model) and 4-bit quantization's memory savings established a viable pathway for on-device medical triage model deployment without compromising clinical safety or performance.
