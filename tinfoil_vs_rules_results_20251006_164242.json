{
  "timestamp": "2025-10-06T17:44:06.659499",
  "progress": "5/5",
  "completed_configs": 5,
  "total_configs": 5,
  "results": [
    {
      "config": {
        "model_name": "tinfoil_agent_no_rag",
        "model_path": "N/A",
        "adapter_path": null,
        "rag_config": null,
        "test_name": "Tinfoil_Agent_NoRAG"
      },
      "timestamp": "2025-10-06T16:50:12.448333",
      "triage_accuracy": 0.17873417721518986,
      "f1_score": 0.054203749619383186,
      "f2_score": 0.09314018977998527,
      "confusion_matrix": [
        [
          353,
          0,
          0,
          0
        ],
        [
          1504,
          0,
          0,
          0
        ],
        [
          118,
          0,
          0,
          0
        ],
        [
          0,
          0,
          0,
          0
        ]
      ],
      "classification_report": {
        "ED": {
          "precision": 0.17873417721518986,
          "recall": 1.0,
          "f1-score": 0.30326460481099654,
          "support": 353.0
        },
        "GP": {
          "precision": 0.0,
          "recall": 0.0,
          "f1-score": 0.0,
          "support": 1504.0
        },
        "HOME": {
          "precision": 0.0,
          "recall": 0.0,
          "f1-score": 0.0,
          "support": 118.0
        },
        "UNKNOWN": {
          "precision": 0.0,
          "recall": 0.0,
          "f1-score": 0.0,
          "support": 0.0
        },
        "accuracy": 0.17873417721518986,
        "macro avg": {
          "precision": 0.044683544303797465,
          "recall": 0.25,
          "f1-score": 0.07581615120274914,
          "support": 1975.0
        },
        "weighted avg": {
          "precision": 0.031945906104790894,
          "recall": 0.17873417721518986,
          "f1-score": 0.054203749619383186,
          "support": 1975.0
        }
      },
      "total_inference_time": 29015.538705587387,
      "avg_inference_time_per_case": 14.691412002829058,
      "cases_evaluated": 1975,
      "success_count": 1975,
      "error_count": 0,
      "error_details": [],
      "unknown_triage_count": 0,
      "total_failures": 0,
      "rag_retrieval_time": null,
      "rag_context_length_avg": null,
      "llm_judge_metrics": {
        "avg_next_step_quality": 76.00759493670886,
        "avg_reasoning_quality": 82.72962025316455,
        "avg_overall_quality": 78.69640506329114,
        "std_overall_quality": 9.21722230807254,
        "min_overall_quality": 36.0,
        "max_overall_quality": 96.8,
        "cases_with_llm_evaluation": 1975
      }
    },
    {
      "config": {
        "model_name": "tinfoil_agent",
        "model_path": "N/A",
        "adapter_path": null,
        "rag_config": {
          "name": "top10_structured_contextual_diverse",
          "chunking_method": "structured_agent_tinfoil_medical",
          "retrieval_type": "contextual_rag",
          "bias_config": "diverse",
          "top_k": 10,
          "description": "Top 10 structured chunking + contextual RAG with diverse sources."
        },
        "test_name": "Tinfoil_Agent_top10_structured_contextual_diverse"
      },
      "timestamp": "2025-10-06T17:05:51.766533",
      "triage_accuracy": 0.18227848101265823,
      "f1_score": 0.06142299320018066,
      "f2_score": 0.09775831437258962,
      "confusion_matrix": [
        [
          353,
          0,
          0,
          0
        ],
        [
          1497,
          7,
          0,
          0
        ],
        [
          118,
          0,
          0,
          0
        ],
        [
          0,
          0,
          0,
          0
        ]
      ],
      "classification_report": {
        "ED": {
          "precision": 0.179369918699187,
          "recall": 1.0,
          "f1-score": 0.3041792330891857,
          "support": 353.0
        },
        "GP": {
          "precision": 1.0,
          "recall": 0.004654255319148936,
          "f1-score": 0.009265387160820648,
          "support": 1504.0
        },
        "HOME": {
          "precision": 0.0,
          "recall": 0.0,
          "f1-score": 0.0,
          "support": 118.0
        },
        "UNKNOWN": {
          "precision": 0.0,
          "recall": 0.0,
          "f1-score": 0.0,
          "support": 0.0
        },
        "accuracy": 0.18227848101265823,
        "macro avg": {
          "precision": 0.29484247967479676,
          "recall": 0.2511635638297872,
          "f1-score": 0.07836115506250159,
          "support": 1975.0
        },
        "weighted avg": {
          "precision": 0.7935785221776269,
          "recall": 0.18227848101265823,
          "f1-score": 0.06142299320018066,
          "support": 1975.0
        }
      },
      "total_inference_time": 54840.194954156876,
      "avg_inference_time_per_case": 27.767187318560442,
      "cases_evaluated": 1975,
      "success_count": 1975,
      "error_count": 0,
      "error_details": [],
      "unknown_triage_count": 0,
      "total_failures": 0,
      "rag_retrieval_time": null,
      "rag_context_length_avg": null,
      "llm_judge_metrics": {
        "avg_next_step_quality": 72.59645569620253,
        "avg_reasoning_quality": 81.67037974683544,
        "avg_overall_quality": 76.22602531645569,
        "std_overall_quality": 9.260887736618189,
        "min_overall_quality": 28.0,
        "max_overall_quality": 96.8,
        "cases_with_llm_evaluation": 1975
      }
    },
    {
      "config": {
        "model_name": "tinfoil_agent",
        "model_path": "N/A",
        "adapter_path": null,
        "rag_config": {
          "name": "top10_structured_pure_diverse",
          "chunking_method": "structured_agent_tinfoil_medical",
          "retrieval_type": "pure_rag",
          "bias_config": "diverse",
          "top_k": 10,
          "description": "Top 10 structured chunking + pure RAG with diverse sources."
        },
        "test_name": "Tinfoil_Agent_top10_structured_pure_diverse"
      },
      "timestamp": "2025-10-06T17:16:42.419209",
      "triage_accuracy": 0.18025316455696203,
      "f1_score": 0.05730561689837426,
      "f2_score": 0.09512054867718145,
      "confusion_matrix": [
        [
          353,
          0,
          0,
          0
        ],
        [
          1501,
          3,
          0,
          0
        ],
        [
          118,
          0,
          0,
          0
        ],
        [
          0,
          0,
          0,
          0
        ]
      ],
      "classification_report": {
        "ED": {
          "precision": 0.17900608519269776,
          "recall": 1.0,
          "f1-score": 0.3036559139784946,
          "support": 353.0
        },
        "GP": {
          "precision": 1.0,
          "recall": 0.0019946808510638296,
          "f1-score": 0.003981420039814201,
          "support": 1504.0
        },
        "HOME": {
          "precision": 0.0,
          "recall": 0.0,
          "f1-score": 0.0,
          "support": 118.0
        },
        "UNKNOWN": {
          "precision": 0.0,
          "recall": 0.0,
          "f1-score": 0.0,
          "support": 0.0
        },
        "accuracy": 0.18025316455696203,
        "macro avg": {
          "precision": 0.29475152129817445,
          "recall": 0.25049867021276595,
          "f1-score": 0.07690933350457721,
          "support": 1975.0
        },
        "weighted avg": {
          "precision": 0.7935134926952012,
          "recall": 0.18025316455696203,
          "f1-score": 0.05730561689837426,
          "support": 1975.0
        }
      },
      "total_inference_time": 69250.94879841805,
      "avg_inference_time_per_case": 35.06377154350281,
      "cases_evaluated": 1975,
      "success_count": 1975,
      "error_count": 0,
      "error_details": [],
      "unknown_triage_count": 0,
      "total_failures": 0,
      "rag_retrieval_time": null,
      "rag_context_length_avg": null,
      "llm_judge_metrics": {
        "avg_next_step_quality": 72.52506329113925,
        "avg_reasoning_quality": 81.65113924050632,
        "avg_overall_quality": 76.17549367088607,
        "std_overall_quality": 9.200467073825234,
        "min_overall_quality": 28.0,
        "max_overall_quality": 96.8,
        "cases_with_llm_evaluation": 1975
      }
    },
    {
      "config": {
        "model_name": "tinfoil_agent",
        "model_path": "N/A",
        "adapter_path": null,
        "rag_config": {
          "name": "top10_structured_pure_diverse",
          "chunking_method": "structured_agent_tinfoil_medical",
          "retrieval_type": "pure_rag",
          "bias_config": "diverse",
          "top_k": 10,
          "description": "Top 10 structured chunking + pure RAG with diverse sources."
        },
        "test_name": "Tinfoil_Agent_top10_structured_pure_diverse"
      },
      "timestamp": "2025-10-06T17:30:24.954624",
      "triage_accuracy": 0.18329113924050633,
      "f1_score": 0.06347382381840845,
      "f2_score": 0.09907603672011496,
      "confusion_matrix": [
        [
          353,
          0,
          0,
          0
        ],
        [
          1495,
          9,
          0,
          0
        ],
        [
          118,
          0,
          0,
          0
        ],
        [
          0,
          0,
          0,
          0
        ]
      ],
      "classification_report": {
        "ED": {
          "precision": 0.17955239064089523,
          "recall": 1.0,
          "f1-score": 0.30444156964208713,
          "support": 353.0
        },
        "GP": {
          "precision": 1.0,
          "recall": 0.005984042553191489,
          "f1-score": 0.011896893588896233,
          "support": 1504.0
        },
        "HOME": {
          "precision": 0.0,
          "recall": 0.0,
          "f1-score": 0.0,
          "support": 118.0
        },
        "UNKNOWN": {
          "precision": 0.0,
          "recall": 0.0,
          "f1-score": 0.0,
          "support": 0.0
        },
        "accuracy": 0.18329113924050633,
        "macro avg": {
          "precision": 0.29488809766022384,
          "recall": 0.25149601063829785,
          "f1-score": 0.07908461580774584,
          "support": 1975.0
        },
        "weighted avg": {
          "precision": 0.7936111361499929,
          "recall": 0.18329113924050633,
          "f1-score": 0.06347382381840845,
          "support": 1975.0
        }
      },
      "total_inference_time": 70187.40353751183,
      "avg_inference_time_per_case": 35.53792584177814,
      "cases_evaluated": 1975,
      "success_count": 1975,
      "error_count": 0,
      "error_details": [],
      "unknown_triage_count": 0,
      "total_failures": 0,
      "rag_retrieval_time": null,
      "rag_context_length_avg": null,
      "llm_judge_metrics": {
        "avg_next_step_quality": 72.63037974683544,
        "avg_reasoning_quality": 81.6040506329114,
        "avg_overall_quality": 76.21984810126582,
        "std_overall_quality": 9.183468414974024,
        "min_overall_quality": 28.0,
        "max_overall_quality": 96.8,
        "cases_with_llm_evaluation": 1975
      }
    },
    {
      "config": {
        "model_name": "rule_based_rag",
        "model_path": "N/A",
        "adapter_path": null,
        "rag_config": {
          "name": "top10_structured_pure_diverse",
          "chunking_method": "structured_agent_tinfoil_medical",
          "retrieval_type": "pure_rag",
          "bias_config": "diverse",
          "top_k": 10,
          "description": "Top 10 structured chunking + pure RAG with diverse sources."
        },
        "test_name": "Rule_Based_RAG_top10_structured_pure_diverse"
      },
      "timestamp": "2025-10-06T17:44:06.659154",
      "triage_accuracy": 0.5215189873417722,
      "f1_score": 0.5451830195549274,
      "f2_score": 0.5046151957448285,
      "confusion_matrix": [
        [
          335,
          18,
          0,
          0
        ],
        [
          809,
          695,
          0,
          0
        ],
        [
          79,
          39,
          0,
          0
        ],
        [
          0,
          0,
          0,
          0
        ]
      ],
      "classification_report": {
        "ED": {
          "precision": 0.2739165985282093,
          "recall": 0.9490084985835694,
          "f1-score": 0.4251269035532995,
          "support": 353.0
        },
        "GP": {
          "precision": 0.9242021276595744,
          "recall": 0.4621010638297872,
          "f1-score": 0.6161347517730497,
          "support": 1504.0
        },
        "HOME": {
          "precision": 0.0,
          "recall": 0.0,
          "f1-score": 0.0,
          "support": 118.0
        },
        "UNKNOWN": {
          "precision": 0.0,
          "recall": 0.0,
          "f1-score": 0.0,
          "support": 0.0
        },
        "accuracy": 0.5215189873417722,
        "macro avg": {
          "precision": 0.2995296815469459,
          "recall": 0.3527773906033391,
          "f1-score": 0.2603154138315873,
          "support": 1975.0
        },
        "weighted avg": {
          "precision": 0.7527557262179533,
          "recall": 0.5215189873417722,
          "f1-score": 0.5451830195549274,
          "support": 1975.0
        }
      },
      "total_inference_time": 25.205859422683716,
      "avg_inference_time_per_case": 0.012762460467181628,
      "cases_evaluated": 1975,
      "success_count": 1975,
      "error_count": 0,
      "error_details": [],
      "unknown_triage_count": 0,
      "total_failures": 0,
      "rag_retrieval_time": null,
      "rag_context_length_avg": null,
      "llm_judge_metrics": null
    }
  ]
}