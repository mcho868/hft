# Section 1 Reorganization - Ready to Execute

## âœ… Dependency Fixes Completed

All file path dependencies have been updated to prevent broken references:

### Fixed Files

| File | Old Path | New Path | Status |
|------|----------|----------|--------|
| `webscrapers/healthify_scraper.py` | `RAGdata/healthify_data` | `RAGdatav3/healthify` | âœ… Fixed |
| `convert_to_mlx_format.py` | `mlx_dataset/triage_dialogues_mlx` | `Final_dataset/triage_dialogues_mlx` | âœ… Fixed |
| `.gitignore` | - | Added backup & index patterns | âœ… Updated |

### Backup Files Created
- `webscrapers/healthify_scraper.py.bak`
- `convert_to_mlx_format.py.bak`

---

## ðŸ“‹ Ready to Reorganize

### What Will Happen

**25 files will be moved to organized backup locations:**

#### Stage 1: Web Scraping Cleanup (6 files)
```
backup_bin/data_quality_checks/
â”œâ”€â”€ check_healthify_consistency.py
â”œâ”€â”€ check_healthify_files.py
â”œâ”€â”€ analyze_healthify_duplicates.py
â”œâ”€â”€ check_source_document.py
â”œâ”€â”€ compare_mayo_ids.py
â””â”€â”€ missing_sources.py
```

**+ RAGdata/ directory â†’ backup_bin/RAGdata_v1/**

#### Stage 2: Dataset Preparation Cleanup (10 files)
```
backup_bin/dataset_analysis/
â”œâ”€â”€ analyze_discrepancy.py
â”œâ”€â”€ analyze_symptoms.py
â”œâ”€â”€ debug_reasoning_issue.py
â”œâ”€â”€ compare_datasets.py
â””â”€â”€ dataset_load.py

backup_bin/dataset_tools_old/
â”œâ”€â”€ dataset_transformer.py
â”œâ”€â”€ redistribute_data.py
â”œâ”€â”€ clean_progress_file.py
â”œâ”€â”€ create_reasoning_dataset.py
â””â”€â”€ prepare_simple_dataset.py

backup_bin/logs/
â””â”€â”€ ai_filtering_detailed_log.txt
```

**+ Final_dataset/triage_dialogues_mlx/ â†’ backup_bin/old_datasets/**

#### Stage 3: Document Chunking Cleanup (3 files)
```
backup_bin/chunking_tools/
â”œâ”€â”€ agent_chunk_deduplicator.py
â””â”€â”€ medical_file_filter.py

backup_bin/chunking_old/
â””â”€â”€ simple_paragraphchunk_maker_from_deduplicated.py
```

**Files to be deleted: ~500MB of regenerable temp data**
- 3 progress files from root
- 12 contextual chunking progress files from backup_bin

#### Stage 4: Vector Index Building Cleanup (6 files)
```
backup_bin/retrieval_old/
â”œâ”€â”€ retrieval_function.py
â”œâ”€â”€ retrieval_function_v2.py
â””â”€â”€ retrieval_testv2.py

backup_bin/demos/
â”œâ”€â”€ basic_retrieval_demo.py
â”œâ”€â”€ contextual_retrieval_demo.py
â””â”€â”€ rag_chat.py

backup_bin/samples/
â””â”€â”€ contextual_retrieval _sample/
```

---

## âœ… Files That Will Remain (Production Code)

### Core Pipeline Scripts
```
webscrapers/
â”œâ”€â”€ nhs_scraper.py                     âœ… (paths fixed)
â”œâ”€â”€ mayo_scraper.py                    âœ…
â”œâ”€â”€ mayo_diagnosis_treatment_scraper.py âœ…
â”œâ”€â”€ healthify_scraper.py               âœ…
â””â”€â”€ medical_scraper.py                 âœ…

Root directory:
â”œâ”€â”€ generate_triage_dialogues.py       âœ…
â”œâ”€â”€ extract_medical_conditions.py      âœ…
â”œâ”€â”€ filter_medical_conditions.py       âœ…
â”œâ”€â”€ ai_filter_medical_conditions.py    âœ…
â”œâ”€â”€ convert_to_mlx_format.py           âœ… (paths fixed)
â”œâ”€â”€ main_chunking_script_v4.py         âœ…
â”œâ”€â”€ main_build_script_index_only.py    âœ…
â””â”€â”€ contextual_retrieval_config.json   âœ…

preparing dataset/
â”œâ”€â”€ simple_deduplicator.py             âœ…
â”œâ”€â”€ clean_triage_data.py               âœ…
â””â”€â”€ prepare_mlx_dataset.py             âœ…
```

### Data Directories
```
webscrapers/                    # All scraper code
RAGdatav3/                      # Source data + chunking scripts
  â”œâ”€â”€ nhs/                      # ~800 condition files
  â”œâ”€â”€ mayo/                     # Comprehensive medical data
  â”œâ”€â”€ healthify/                # NZ-specific health info
  â””â”€â”€ scripts/                  # Modular chunking framework (10 files)

RAGdatav4/                      # Chunked documents
  â”œâ”€â”€ *_chunks_*.json           # ~100 chunk files
  â””â”€â”€ indiv_embeddings/         # FAISS indices (~100-120 .index files)

Final_dataset/
  â”œâ”€â”€ final_triage_dialogues_mlx/    # âœ… Production training data
  â”‚   â”œâ”€â”€ train.jsonl (9,100 dialogues)
  â”‚   â”œâ”€â”€ valid.jsonl (1,975 dialogues)
  â”‚   â””â”€â”€ test.jsonl (1,975 dialogues)
  â”œâ”€â”€ generated_triage_dialogues.json
  â”œâ”€â”€ simplified_triage_dialogues_*.json
  â””â”€â”€ ...

preparing dataset/              # Core dataset scripts only (3 files)
```

### Reference Files
```
unique_medical_conditions.txt
ai_filtered_medical_conditions.txt
filtered_medical_conditions.txt
```

---

## ðŸŽ¯ Expected Outcome

### Clean Project Structure
- **16 production scripts** in root and webscrapers/
- **3 core dataset scripts** in preparing dataset/
- **10 modular chunkers** in RAGdatav3/scripts/
- **100+ chunk files** in RAGdatav4/
- **100+ FAISS indices** in RAGdatav4/indiv_embeddings/
- **13,000 training dialogues** in Final_dataset/

### Organized Backups
- **11 backup categories** in backup_bin/
- **25+ experimental/debug files** preserved but out of the way
- **Old datasets** archived for reference

### Space Savings
- **~500MB** of regenerable temp files deleted

---

## ðŸš€ Execute Reorganization

### Option 1: Review First (Dry Run)
```bash
./reorganize_section1.sh --dry-run
```
Shows exactly what would happen without making changes

### Option 2: Execute Now
```bash
./reorganize_section1.sh
```
Performs the reorganization

---

## ðŸ”„ Rollback Plan

If needed, all original files are backed up:
- Script backups: `*.bak` files
- Directory backups: `backup_bin/RAGdata_v1/`, `backup_bin/old_datasets/`
- Moved files: Organized in `backup_bin/*/`

To rollback:
```bash
# Restore from backup
mv webscrapers/healthify_scraper.py.bak webscrapers/healthify_scraper.py
mv convert_to_mlx_format.py.bak convert_to_mlx_format.py

# Move files back from backup_bin if needed
# (files are preserved, not deleted)
```

---

## âœ… Verification Checklist

After reorganization, verify:
- [ ] All scrapers still work (`python webscrapers/nhs_scraper.py --test`)
- [ ] Dataset generation works (`python generate_triage_dialogues.py` - check imports)
- [ ] Chunking works (`python main_chunking_script_v4.py` - select 1 config to test)
- [ ] Index building works (`python main_build_script_index_only.py`)
- [ ] Fine-tuning scripts find data (`ls Final_dataset/final_triage_dialogues_mlx/`)

---

## ðŸ“Š Statistics

| Metric | Count |
|--------|-------|
| Files to move | 25+ |
| Files to delete | 15 (temp files) |
| Directories to backup | 3 |
| Backup categories | 11 |
| Production scripts kept | 29 |
| Data directories kept | 5 |
| Space freed | ~500 MB |

---

**Ready to proceed?**

Run `./reorganize_section1.sh` to execute the reorganization.

All dependencies have been fixed and verified âœ…
